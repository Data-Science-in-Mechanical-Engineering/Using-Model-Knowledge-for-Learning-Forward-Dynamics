'''
This script reads the raw data generated by the KUKA robot simulation and 
prepares the training and test datasets.

The following steps are done:
    1. add noise to the acceleration measurements
    2. select training dataset with Farthest Point Samping (FPS) algorithm (from PointNet++ paper)
    3. double-check if the acceleration measurements respect the surface constraint
    4. save training and test datasets

'''
import os, sys, importlib
sys.path.append( os.path.join( os.path.dirname(__file__), os.path.pardir ) )

import math
import time
from collections import ChainMap, namedtuple
from datetime import date, datetime
from enum import Enum, auto
from typing import List
import dill
from copy import copy, deepcopy
import matplotlib.pyplot as plt
import numpy as np
from numpy.linalg import solve, inv, pinv
import scipy
from scipy import interpolate, signal
from scipy.cluster.vq import vq, kmeans2, whiten
from torch_cluster import fps
from addict import Dict
# import pybullet as p
from tqdm import tqdm
import torch
from utils.StructArray import StructTorchArray, StructNumpyArray
import sgp.sgp as sgp



# setup print options
np.set_printoptions(precision=4,threshold=1000,linewidth=500,suppress=True)
torch.set_printoptions(precision=4,threshold=1000,linewidth=500)

# clean GPU cache
sgp.cleanGPUcache()

# ! VERY IMPORTANT: change torch to double precision
torch.set_default_tensor_type(torch.DoubleTensor)

# select choosen device if available
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
print(f'\nUsing device: {device}')


''' ----------
 Config
 ----------- '''

# import configurations
cfg = importlib.import_module('results.KUKA-surf-dataset.config_KUKA')


''' ---------- 
 Load raw simulation data
 --------- '''
# load sate logs
with open(cfg.log.resultsFileName_raw, 'rb') as f:
	data = dill.load(f)

dt = data.dt
statelogs = data.statelogs.to(device)

''' -----
 Load MultiBodyDynamics model, that generated the data.
 ----- '''

mbdKuka = sgp.loadKUKA(
    urdfPath = cfg.kuka.urdf_filename, 
    basePos = cfg.kuka.basePos, 
    baseOrn = cfg.kuka.baseOrn, 
    F_r_FE = cfg.ctrl.F_r_FE, 
    gravity = cfg.sim.gravity, 
    surf_fun   = cfg.surftorch.fun, 
    surf_fun_J = cfg.surftorch.fun_J, 
    surf_fun_H = cfg.surftorch.fun_H,
    endEffectorName = cfg.kuka.endEffectorName,
    baumgarte_wn  = cfg.contact_dynamics.baumgarte_wn,
    baumgarte_ksi = cfg.contact_dynamics.baumgarte_ksi
).to(device)

''' -------
 Select data from different time steps 
 -------- '''

d   = statelogs[:-1]
d_n = statelogs[1:]

dataset = StructTorchArray(
    t = d.t, k = d.k,                               # time and iteration
    q = d.q, dq = d.dq, tau = d.tau,                # inputs
    ddq = d_n.ddq, qn = d_n.q, dqn = d_n.dq,        # outputs
    M = d.M, f = d.f, g = d.g, A = d.A, b = d.b,    # extra
    contacts = torch.min(d.contacts,d_n.contacts)   # contact detection
)

# discard initial an final data
dataset = dataset[1000:-1000]   

assert torch.all( dataset.contacts ), 'Data seems strange. The end-effector is not always in contact with the surface'



''' -----
 Add Noise
 ----- '''

mean = torch.zeros_like(dataset.ddq)
std  = torch.ones_like(dataset.ddq) * cfg.ds.ddq.noise.std

dataset.ddq_raw = dataset.ddq
dataset.ddq     = dataset.ddq_raw + torch.normal(mean=mean, std=std )


''' -----
 Select training dataset
 For GP regression we can only train with a dataset with limited size. 
 So we want a dataset as informative as possible
 ----- '''

print('Preparing training dataset...')

# prepare features
feats = torch.cat((dataset.q, dataset.dq, dataset.tau ), axis=-1).to(device)
feats_n = (feats-feats.mean(0))/feats.std(0)

# run clustering: Farthest Point Sampling
ratio = cfg.ds.datasetsize_train/len(feats_n)
index = fps(feats_n, ratio=ratio, random_start=True)

# select training dataset
dataset_train = dataset[index].to(device)
# print('...finished')

# print('Generating dynamics matrices for training dataset...')
dataset_train = sgp.get_KUKA_SGPMatrices_from_MDB( mbd=mbdKuka, dataset=dataset_train )
print('...finished')



''' ----------
 Check if acceleration measurements respect the constraints 
 ----------- '''

constraintError = (torch.einsum('nij,nj->ni', dataset_train.A, dataset_train.ddq) - dataset_train.b).cpu().numpy().flatten()

fig,ax = plt.subplots(1,2,figsize=(10,4))
ax[0].grid(True)
ax[0].scatter( dataset_train.t.cpu(), constraintError )
ax[0].set_xlabel('time [s]')
ax[0].set_ylabel('Constraint error A*ddq-b')
ax[1].hist(constraintError, bins=200, range=np.percentile(constraintError,[5,95]))
if cfg.log.saveImages: fig.savefig( cfg.log.addFolder('constraintError.pdf'), dpi=cfg.log.dpi)
if cfg.log.showImages: plt.show()

# constraint error must be somewhere around zero. Otherwise there is a bug in the code
assert np.abs(np.mean(constraintError)) < 0.01, 'Constraint Violation mean is too high... please double check the code or the simulation'


''' -----
 Select test dataset (selected blocks of 1000 points)
 ----- '''

idxWhereTestDatasetStarts = [1000, 3000, 7000, 10000] # [5000, 10000, 15000, 20000, 25000, 30000, 40000]

print('Preparing test dataset...')
dataset_test_list = []

for idxStart in tqdm(idxWhereTestDatasetStarts):
    # subsample each measurement data (1 sample every 0.01s) and select only the first 15 seconds from each file.
    # this limits the amount of samples and allows us to have batches of test datasets that fits into memory
    dataset_test_i = copy(dataset[idxStart:idxStart+int(15/dt)+1:math.ceil(0.01/dt)])
    # move to device to speed up MBD computations
    dataset_test_i.to(device)
    # generate aditional matrices
    dataset_test_i = sgp.get_KUKA_SGPMatrices_from_MDB( mbd=mbdKuka, dataset=dataset_test_i )
    # append to the list
    dataset_test_list.append( dataset_test_i )
print('...finished')
    

''' -----
 Check Dataset validity
 ----- '''

fig, axs = plt.subplots(3,1,figsize=(14,8))
for i, (d,f) in enumerate(zip(dataset_test_list, idxWhereTestDatasetStarts)):
    ddc = torch.einsum( 'bij,bj->bi' , d.A, d.ddq ) - d.b
    dc  = torch.einsum( 'bij,bj->bi' , d.A, d.dq )
    c   = d.I_c
    axs[0].plot( d.t.cpu().numpy(), ddc.cpu().numpy(), 'o', label=str(f))
    axs[1].plot( d.t.cpu().numpy(),  dc.cpu().numpy(), 'o', label=str(f))
    axs[2].plot( d.t.cpu().numpy(),   c.cpu().numpy(), 'o', label=str(f))
axs[0].set_ylabel('$\ddot c (q,\dot q, \ddot q)$')
axs[1].set_ylabel('$\dot c (q,\dot q)$')
axs[2].set_ylabel('$c(q)$')
axs[2].set_xlabel('time [s]')
axs[0].grid(True)
axs[1].grid(True)
axs[2].grid(True)
plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1))
plt.tight_layout()
if cfg.log.saveImages: fig.savefig( cfg.log.addFolder('constraintError_testdataset.pdf'), dpi=cfg.log.dpi)
if cfg.log.showImages: plt.show()


from copy import deepcopy
dataset1 = deepcopy(dataset_test_list[0]).to(device)
dataset2 = deepcopy(dataset_test_list[0]).to(device)
dataset2 = sgp.get_KUKA_SGPMatrices_from_MDB(mbdKuka, dataset2)

print(torch.norm( dataset1.M - dataset2.M ))
print(torch.norm( dataset1.A - dataset2.A ))
print(torch.norm( dataset1.b - dataset2.b ))
print(torch.norm( dataset1.f - dataset2.f ))
print(torch.norm( dataset1.g - dataset2.g ))

ddq_mbd = torch.einsum('nab,nb->na', dataset1.L, dataset1.b) + torch.einsum('nab,nbc,nc->na', dataset1.T, dataset1.Minv, dataset1.f + dataset1.g + dataset1.tau)

fig, ax = plt.subplots(data.nq, 1, sharex=True, constrained_layout=True)
for n in range(data.nq):
    ax[n].grid(True)
    ax[n].plot( dataset1.t.cpu(), dataset1.ddq_raw[:,n].cpu(), label='sim' )
    ax[n].plot( dataset1.t.cpu(), ddq_mbd[:,n].cpu(),       '--', label='mbd' )
    ax[n].set_ylabel(f'ddq_{n}')
ax[-1].set_xlabel('time')
ax[0].legend()
plt.suptitle('Comparison $\ddot q_{simulation}$ and $\ddot q_{mbd}$')
fig.tight_layout(pad=0.1)
if cfg.log.saveImages: fig.savefig( cfg.log.addFolder('ddq-mbd-and-sim-comp.pdf'), dpi=cfg.log.dpi)
if cfg.log.showImages: plt.show()


fig, ax = plt.subplots(data.nq, 1, sharex=True, constrained_layout=True)
for n in range(data.nq):
    ax[n].grid(True)
    ax[n].plot( dataset1.t.cpu(), dataset1.ddq[:,n].cpu(), label='sim' )
    ax[n].plot( dataset1.t.cpu(), ddq_mbd[:,n].cpu(),       '--', label='mbd' )
    ax[n].set_ylabel(f'ddq_{n}')
ax[-1].set_xlabel('time')
ax[0].legend()
plt.suptitle('Comparison $\ddot q_{simulation}$ and $\ddot q_{mbd}$')
fig.tight_layout(pad=0.1)
if cfg.log.saveImages: fig.savefig( cfg.log.addFolder('ddq-mbd-and-sim-comp_with_noise.pdf'), dpi=cfg.log.dpi)
if cfg.log.showImages: plt.show()


''' -----
 Select test dataset for long term prediction 
 ----- '''

# select 4 starting points at random
idxWhereTestDatasetStarts = [7000, 13000, 21000, 33000]

print('Preparing test dataset for long term prediction...')
dataset_longTerm_list = []

for idxStart in tqdm(idxWhereTestDatasetStarts):
    dataset_test_i = copy(dataset[idxStart:idxStart+int(10/dt)+1])       # select 10s of consecutive data
    # append to the list
    dataset_longTerm_list.append( dataset_test_i )
print('...finished')



''' -------
 Save datasets
 ------- '''

data = Dict()
data.dataset_train          = dataset_train
data.dataset_test_list      = dataset_test_list
data.dataset_longTerm_list  = dataset_longTerm_list
data.nq                     = mbdKuka.nq
data.dt                     = dt
data.contact                = True

if cfg.log.saveSimResults:
	with open( cfg.log.resultsFileName, 'wb') as f:
		dill.dump( data, f )
	print(f'\nSAVED: {len(dataset_train)} training data and lots of test data points saved to \n{f.name}\n')

